\documentstyle[12pt]{article}

\title{Information extraction --- first notes}
\author{Michael Nielsen}

\begin{document}

\maketitle

This document contains my first notes getting a survey overview of the
field of information extraction, including notions such as entity
recognition.  It's unrevised and undistilled, written solely for
myself, and unlikely to be of much use to anyone else.  If for some
reason you want to look through the notes, that's fine, but they will
contain lots of errors, misunderstandings, thinking out loud, and so
on.

\section{Overview}

\textbf{Wikipedia article on information extraction:} Example is
extracting reports of company mergers from news wire articles.  Early
example of a system was JASPER, built for Reuters, with the aim of
providing financial news to financial traders.  1987-1998: the Message
Understanding Conferences were seminal.  Support came from DARPA, who
wished to automate tasks performed by analysts.  It's unclear to me
how much the semantic web will help --- stuff in ``machine readable''
form often isn't that much easier to understand than plain text.
Standard way of thinking: unstructured data -> database.  There are
all sorts of subtasks: recognizing named entities; detecting
coferences; extracting relationships.  Not so dissimilar to full-on
natural language processing! Also includes things like table
extraction, comment extraction, and terminology extraction.
Information extraction is also done on media sources other than text.
Wrappers can be used to extract a particular page's content.  They're
usually taylored to highly structured collections of pages.

The article claims that there are \textbf{three standard approaches}
to information extraction: hand-written regular expressions;
classifiers (Bayes, Maxent); and sequence models.

\textbf{Wikipedia article on text simplification:} This is a procedure
one applies to a text.  The intent is to preserve meaning, while
making the text simpler.  It'd be interesting to have an example of
output from such a system.  One thing which is done is to break
complex compound sentences down into simple sentences, without
subordinate clauses.

At least some of this work is being used in science, to extract
information from papers.

The use of text simplification sets up a pipeline.  We take complex
sentences, then apply text simplification to produce simple output
sentences, which can then be understood using a further stage.  It's a
nice two-part model.  It's unclear to me that this is what we actually
do when we parse speech (not that that necessarily matters).  One of
the papers in the area (by Klebanov, Knight and Marcu) introduces a
notion of Easy Access Sentences: the goal of text simplification is to
produce such sentences.

\textbf{Wikipedia article on wrappers:} Says there are two main
approaches to wrappers: (1) wrapper induction, and (2) automated data
extraction.  Wrapper induction starts with a set of manually labelled
training examples to learn data extraction rules.  The problem, of
course, is that this requires one to come up with a set of training
examples, and the system will break when we see examples outside the
types seen in the training set.  This may occur, for example, when a
site changes its template.  Automated data extraction tries to
discover the patterns in webpages without manually labelled training
data.

\textbf{Wikipedia article on named entity recognition:} Also known as
entity identification and entity extraction.  The idea is to classify
atomic elements in text into predefined (really?) categories.  You can
think of it as a form of annotion, taking unstructured text, and
beginning to structure it.  For example, the Message Understanding
Conferences developed a set of standard tags (ENAMEX) that were used
to categorize the atomic elements in text.  This approach seems
surprising to me: it doesn't seem scalable, given the very large
(infinite) number of categories we humans potentially have available.
It's also true that one atomic element in text may belong to multiple
categories.

The article claims that the best current systems for named entity
recognition perform at near-human levels of performance.  But it also
comments that named entity recognition systems which perform well in
one domain often don't perform well in other domains.  As for machine
translation there is again a distinction between rule-based and
statistical model-based systems.  (I wonder which is better?)

At least two hierarchies of named entities have been proposed in the
literature.  There are BBN categories, with 29 types and 64 subtypes.
There is an extended hierarchy, due to Sekine, with 200 subtypes.

\textbf{Wikipedia article on Message Understanding Conferences:} The
conference was organized with many research teams competing against
one another, with a metrics-based approach.  At the sixth conference
--- MUC-6 --- the task of recognizing named entities and coreferences
was added.  For a named entity, all phrases in the text were ``to be
marked as person, location, organization, time or quantity.''  The MUC
conferences were associated with DARPA's TIPSTER program, which aimed
at doing document detection, information extraction, and
summarization.  TIPSTER was apparently discontinued in 1998.

\textbf{Wikipedia article on precision and recall:} I'm familiar with
these notions in the context of information retrieval.  But the
Wikipedia article does an interesting job broadening the discussion to
pattern recognition in general.  Starts with the notion of
\emph{accuracy}: how often is the correct results returned.  For
example, if we're shown an image of a whale, how often does an
algorithm correctly identify it as a whale?  Considers precision and
recall as generalizations.  I'll come back to this point.

Let's recap how it works in information retrieval.  You're given a
query, $q$, and identify some set $S$ of documents as relevant.  In
fact, the ``true'' set of relevant document is $S_T$.  The ``correct''
documents are those in $S \cap S_T$.  So \emph{recall} is the fraction
of all relevant documents returned: $|S \cap S_T| / |S_T|$.
\emph{Precision} is the fraction of document returned by the search
which are truly relevant: $|S \cap S_T| / |S|$.

In other words, recall is the answer to the question ``What fraction
of truly relevant documents to the query are returned?''  Precision is
the answer to the question ``What fraction of the returned documents
are truly relevant to the query?''

It's easy to build an information retrieval system which gets high
recall: it simply dumps everything.  Unfortunately, this will gives
low precision: most of the documents won't be truly relevant to the
query.

It's also (relatively) easy to build an information retrieval system
which gets high precision: it simply tries to identify a single
document which is highly likely to be relevant, and returns that.  

Note that the task of building high-precision systems is complicated
by the fact that some queries are going to be hard to understand.
Many people might guess that ``President in a black hat'' refers to
Abraham Lincoln, but an information retrieval system might have
trouble with this.  So it seems likely that we might get low-precision
results.  But a sufficiently generous search algorithm might also get
pretty high recall, since at least \emph{some} webpages will make the
connection between ``black hat'' and ``Lincoln'', and (hopefully) that
will allow it to infer that it should return other pages connected to
Lincoln.

It's worth pointing out that both precision and recall are
mathematically \emph{ill-defined}.  They depend on human psychology.
And to evaluate them will require training examples.  It is by no
means clear how one would do this at web scale.

We can think of spam in this context.  What spam does is pollutes your
search results, returning irrelevant pages, i.e., decreasing
precision.  But it doesn't necessarily affect recall.  When someone
says something is ``spammy'' that's really what is meant: it is
decreasing precision.  ``Good'' advertising doesn't do this: it only
shows you stuff that is relevant, while ``bad'' advertising decreases
precision.

It's interesting to consider that television advertising is very low
precision, while Google's advertising aims to be high precision: i.e.,
to only show you stuff that is actually relevant.

Coming back to the issue of evaluating precision, this is something
which in principle we ought to be able to estimate using user click
data.  It's not so clear, though, how we'd be able to estimate recall.
My intuition is that users will be much more sensitive to changes in
precision than to changes in recall.  

There is a use case where the last sentence in the last paragraph is
definitely not true: when you're trying to find something that you've
previously found.  In that case, there is only one relevant document,
and it should (ideally) be the single document returned.  At a minimum
you need perfect recall (and, ideally, high precision).

Note that for search, precision is not really a precise enough notion
(!)  The problem is that we intuitively have notions of more and less
relevant: we want highly relevant things to come up early in the
search results, and less relevant things to come up later.  I don't
know how to quantify this.

With the above discussion in mind, it starts to become obvious how
this applies in more general classification contexts.  Suppose we have
a large number of photographs.  We're asked: which ones are whales?
We have an algorithm attempt a classification.  Recall is the fraction
of (true) whales it correctly ``recalls'', i.e., identifies as whales.
Precision is the fraction of what it recalls that are, in fact,
whales.  Ideally, we'd score perfecly on both (i.e., 1.0).  But it's
pretty clear that we'd be happy with high precision (so nearly
everything it identifies is, in fact, a whale), and low recall (so it
misses some whales).

More generally, for any classification tasks, the precision is the
fraction of \emph{positives} which are \emph{true}: i.e., the fraction
of positives (``this is a whale'') which are actually correct (``this
really is a whale'').  The recall is the fraction of the \emph{true}
things (``this really is a whale'') which are in positive class
(``this is a whale'').

The article remarks that there is often an inverse relationship
between precision and recall. We can increase recall by being more
generous about what documents to return from our search: but we
decrease precision (more false positives).  We can increase precision
by being very strict about what documents to return, but lowering
recall (we'll miss some, getting more false negatives).

Apparently, the two are often combined into a single measure, such as
the F-measure, or the Matthews correlation coefficient.  It's also
often the case that in evaluations one tries to fix one quantity, and
see how to vary the other.

Note that intuitively these notions can be put into good
correspondence with what it's like to talk with someone about a
subject: someone has very good recall if they can tell us everything
relevant to some subject.  But some people with very good recall may
nonetheless have low precision: they often tell us things which aren't
really relevant.  The intuitive matchup with pattern recognition and
classification isn't so good.

Apparently, in binary classification problems, recall is sometimes
called sensitivity.  This is a good matchup to our intuition: a system
may be highly sensitive to pictures of whales, for example.

The article also defines these notions using the ``true positive'',
``false positive'' etc terminology.  This is worth getting comfortable
with, and so I'll go through it here.  Note that a false negative is
something that is incorrectly classified as a negative; e.g., it
\emph{is} a photo of a whale, but is incorrectly classified as not
being one.  So:
$$
\mbox{recall} = \frac{tp}{tp+fn}, 
$$ 
since $tp+fn$ is the total number of true instances (whales)1.
Meanwhile,
$$
\mbox{precision} = \frac{tp}{tp+fp},
$$
since $tp+fp$ is the total number of things identified as being whales
by the classifier, regardless of whether they truly are.

The article points out a nice probabilistic interpretation.  The
recall is the probability that a randomly chosen relevant document
will be returned in the search.  The precision is that probability
that a randomly chosen retrieved document is, in fact, relevant.

Of course, all of this is ``just'' terminology.  However, it is worth
mastering becasue the terminology is so widely used within the field.
With that said, the terminology imposes one particular way of looking
at the world, and other ways are possible.  It's worth keeping closely
in mind that there is a much more fundamental underlying question:
``What documents should a search system return'', and ``What instances
should our classifier return as positives?''  It's important to keep
going back to these more fundamental questions, and to answer them
anew; recall and precision are only proxies for answers, albeit
widely-used proxies.

There is a nice quote at the end of the article that points out that
for web browsing purposes neither high recall nor high precision may
be of all that much use: all we really want are some relevant
documents in the top ten search results.  Now, in practice there may
be other use cases --- e.g., when we want to find a specific page ---
but this is certainly the case often.

\textbf{Wikipedia article on coreference:} ``Michael said he would run
to the store'': in this sentence ``Michael'' and ``he'' both refer to
the same thing: they are \empph{coferent}.  Note that references are
frequently ambiguous.  In context, ``he'' might actually refer to
someone else, if the sentence were part of a larger conversation.

Pronouns obviously need to have their referent disambiguated.  

There are also instances of synonomous noun phrases: ``IBM'' and ``Big
Blue'' will often have the same referent.

I can see why Lisp is so popular with some in the AI community: we
don't need so much algorithms as meta-algorithms in order to think
about these kinds of problems.  Any given approach will have
limitations; what we want is systems that get better in response to
feedback.

The article claims there is a tradeoff between precision and
recall. In this context, I don't quite get it.  Surely all that
matters is precision: we want to make sure that our phrase, say
``he'', refers to the right referent, say ``Michael''.  So it would
seem that precision is all. 

Apparently ``it'' causes all sorts of problems for coreference
resolution, just because it (!) can be used in so many ways.

The article claims that algorithms for co-reference resolution tend to
have accuracy in the 75 percent range --- I assume this is a reference
to precision.

\textbf{Wikipedia article on relationship extraction:} The article is
rather incomplete at present.  It focuses on domain ontologies, and
mentions the extraction of such ontologies from extant sources (e.g.,
Wikipedia).

\textbf{Wikipedia article on Conditional Random Fields:}

\textbf{Wikipedia article on disambiguation:}

\textbf{Wikipedia article on terminology extraction:}

\textbf{Wikipedia article on supervised learning:}

\textbf{Wikipedia article on unsupervised learning:}

\textbf{Wikipedia article on naive Bayes:}

\textbf{Wikipedia article on Hidden Markov Model:}


Other Wikipedia articles: concept mining; web scraping; nutch;
enterprise search; semantic translation; TIPSTER; faceted search;
natural language processing; Matthew Correlation Coefficient. F1
score.  Type I and Type II errors.  Pleonasm and it.  Data warehouse.
Business Intelligence [2.0].

\section{Papers}

What are the key papers?  It's worth also sorting out some of the key
journals, key people, key institutions.

\textbf{Motivations and methods for text simplification:}
(R. Chandresekar, Christine Doran, and B. Srinivas, 1996):
http://portal.acm.org/citation.cfm?id=993361.  This appears to be one
of the key early papers.  The idea is that instead of parsing a
sentence, we'll instead use an alternative approach which is simpler
than full parsing.  They present two approaches: (1) Use a finite
state group to produce noun and verb groups; (2) Use a supertagging
model to produce dependency linkages.

\textbf{Ralph Grisham and Beth Sundheim, Message Understanding
  Conference - 6: A Brief History:
  http://acl.ldc.upenn.edu/C/C96/C96-1079.pdf:} A metrics-based
approach to driving science.  It would be interesting to know how well
this worked (it's a very clear goal!) and what it missed (may have led
to a concentration of attention).  MUC-1 is described as being
exploratory.  MUC-2 crystallized things out.  The first metric is
\emph{recall}: what fraction of keys were filled out correctly?  And
\emph{precision:} what fraction of keys that were filled out were
filled out cooectly?  (Note that precision will always be better than
recall with this definition.)  This article suggests that it wasn't
until MUC-5 that it became part of DARPA TIPSTER.  Interestingly,
MUC-6 broadened the metrics, so that there were several tasks
participants could focus on.

\textbf{Yifan Li, Petr Musilek, Marek Reformat, and Loren Wyard-Scott:
  ``Identification of pleonastic it using the web'' (2009):
  http://www.jair.org/media/2622/live-2622-4362-jair.ps:} Apparently
this paper makes use of the web as a corpus to do an excellent job of
identifying pleonastic uses of ``it'' in sentences.  This can help
with coreferent resolution.  More generally, this seems like an
interesting and powerful idea: using the web as training data to help
in other tasks.

\textbf{Oren Etzionia, Michele Banko, Stephen Soderland, and Daniel
  S. Weld: ``Open information extraction from the web'' (2008):
  http://portal.acm.org/citation.cfm?id=1409378} Starts with the idea
that instead of returning search results, we'll get an automatically
synthesized overview of what we need to know.  It's an interesting
motivating vision, but may not be a realistic endpoint.  It seems that
working on the web is both a blessing and a cure.  It's a curse
because of the open nature --- anyone can post, and so there is a lot
of misleading information.  It's a blessing, though, because of the
enormous scale, which gives us lots of data to use.

``At the core of an IE [information extraction] system is an
extractor: it overlooks irrelevant words and phrases and attempts to
home in on entities and the relationships between them.''  Example:
``Paris is the stylish capital of France'' might get mapped to (Paris,
Capital, France).  Note that this example does lose something --- we
lost the information about Paris being stylish --- but we also gained
something in terms of being able to make inferences.  

The article identifies three broad methods for information extraction:
knowledge-based methods; supervised learning; unsupervised learning.

The knowledge-based methods required the use of handcrafted rules,
tailored to particular domains.  This would obviously require a lot of
work.  It was popular in the early MUC conferences, which tended to
narrow the domain of considered documents.  Not portable, not easy to
scale, without enormous human effort.

Identifies modern IE, beginning with the work of Soderland, Riloff and
Kim and Moldovan, in the 1990s.  These use supervised learning.
Identifies some recent advances, including automatic induction of
features when learning conditional random fields, and using Markov
logic networks to develop extraction frameworks.  

Identifies KnowItAll as ``the next step'': uses a small number of base
rules to figure out how to label its own training examples.  Doesn't
require domain-specific rules.  It's based on a very clever idea:
using search to train the system.  So, for example, we look for pages
containing statements like ``X is a country''.  The results are not
entirely reliable: I just tried ``* is a country'' in Google, and the
first three results tell me that Africa, Europe and Dondal Rumsfeld
are all countries.  This is a problem: when are people speaking
polemically, or ironically, or figuratively?

Next, having identified some candidates, we then query the web to
figure out just how likely each one is to be correct.  Apparently, the
rules are constructed so that KnowItAll \emph{iteratively bootstraps}
its own learning process.

KnowlItAll requires the set of relations to be named by the human user
in advance.  

It seems that bootstrapping is really helped by starting with a fairly
trustworthy corpus.  It suggests an approach: start with something
very trustworthy, and then gradually branch out, adding more and more
sources, cross-checking and extending in this way. 

Something that I'm noticing as I read is just how important
classifiers are.  Much of what we're doing is building up a huge array
of classifiers.  These let us answer membership questions: Is a
labrador a dog?; Is a sheep a plant?; Is a colour a rectangle?  And so
on.

We have all these abstractions for dealing with these things.  And yet
the abstractions are not yet powerful enough to support the tasks that
we humans do effortlessly.

Often, we apply classifiers in parallel.  In the sentence ``Garth
Brooks is a country singer'', should think of Garth Brooks as a
country, or as something else.  If we have two possible
classifications --- country and country singer --- then we should
detect both, and then do some kind of resolution.  So it's a two-stage
process: find possible classifications, then do conflict resolution.
That problem of conflict resolution is also likely to be quite
complex.

Let me do some review of what's involved: learning of rules,
bootstrapping from a starting corpus.  One thing we do is learn what
the enties / concepts are.  Then, later, we need to do entity
extraction, and that may involve first figuring out the possible
entities, and then going through a disambiguation / conflict
resolution phase.

Next, they talk about \emph{open information extraction}: extraction
from open-ended sources, like the web.  Some points to note: they can
do this in any language; they don't rely on having a set of relations
upfront, but rather infer the relations from the corpus; it's not
clear what the role of punctuation is, but clearly it's important.

Something I find very striking in all this is how much it relies on
the subject-verb-object way of expressing knowledge.  That is,
obviously, tremendously powerful, but it's not clear to me that it's
really powerful enough.  Still, it's also the kind of thing that seems
useful to explore and develop.

Something striking about all this is that they're not talking much
about performance, or how they assess it, beyond occasional mentions
of precision and recall, without numbers.  This seems problematic.

TextRunner learns the entities, relations, and classes (?) from
scratch.  It uses conditional random fields.  The idea seems to be to
build some sort of model by maximizing conditional probabilities in
some way.  I'd need to see details, but this general idea seems
attractive to me.  They have something called the Resolver system
which figures out the probability that two strings are synonomous.

They have run TextRunner on a collection of 120 million web pages,
extracting 500 million tuples.  The precision of the process exceeds
75 percent --- in other words, more than 75 percent of the tuples are
actually correct!  Increasing the size of the corpus apparently
increases precision.  They say it also increases recall, although in
this context I'm not sure what that means.

Applications of open information extraction: they provide three:
question answering; opinion mining; and fact checking.  I can see how
any of these might be useful in conjunction with human operators.  I
like their proposal for a fact checker built into a word processor!

I wonder how the relations they extract compare to Cyc?

Summing up: powerful ideas include the notion of doing some sort of
disambiguation; using the web as an information source to extract
relations; using Markov chain models and statistical models to make
inferences about possible relations.  In short, I've got something of
a map to think about, but really need to dig down into some of the
details.

\textbf{Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland,
and Mausam (2011): Open Information Extraction: the Second Generation}

\textbf{Information extraction (Sunita Sarawagi):
http://portal.acm.org/citation.cfm?id=1498845}

\textbf{Coupled semi-supervised learning for information extraction (Andrew
Carlson, Justin Betteride, Richard C. Wang, Estevam R. Hruschka, Jr,
and Tom M. Mitchell): http://portal.acm.org/citation.cfm?id=1718501}

\textbf{Open Information Extraction (Oren Etzioni):
http://oai.dtic.mil/oai/oai?verb=getRecord\&metadataPrefix=html\&identifier=ADA538482}

\textbf{More sources: http://ai.cs.washington.edu/projects/open-information-extraction}

\textbf{Banko and Brill:}

\textbf{Palantir blog:}

\section{Patents}


\section{Concepts and questions} 

Entity-relationship diagrams.

\section{Tools}

What are the standard tools?

\textbf{Apache UIMA:}

\textbf{AlchemyAPI:}

\textbf{NetOwl Extractor:}

\textbf{Tools on Github: matpalm/named-entity-extraction XXX}

\textbf{ReVerb:}

\textbf{GExp:}

\textbf{OpenCalais:}

\textbf{Mallet:}

\textbf{DBpedia Spotlight:}

\textbf{CRF implementations:}

\textbf{General article for text engineering:}

Stanford CRF-based named entity recognition system.

Apache OpenNLP.

Cyc.  OpenCyc.

Palantir. 

Wolfram Alpha.

KnowItAll

IntelligenceInWikipedia project

Weka

Guava

TextRunner

\section{Miscellanea}

\textbf{On silver bullets:} A large number of different ideas and
algorithms have been used to do information extraction.  Are there
silver bullet ideas that can be used to solve the information
extraction problem at one go?  It's certainly true that there are some
\emph{very good} ideas that let us go far with surprisingly little
effort.  However, human language has evolved over many thousands of
years, and has a large amount of inherent complexity, a complexity
which is in part reflected by the many years we spend learning to
understand speech, and by the amusing ``obvious'' (actually, not at
all) mistakes that children often make.  It may be that any system for
extracting information thus has to reflect this inherent complexity.
We should aim for big wins, but expecting a silver bullet may be
expecting too much.

\textbf{On dealing with uncertainty and issues of credibility:} When
we do an extraction, e.g., ``Paris is the stylish capital of France''
-> (Paris, CapitalOf, France), we should associate a probability to
the resulting assertion.  Actually, we probably need a model of our
source as well: Is the source trustworthy?  Are they a liar?  How
could we tell?  Maybe by comparing with other information?

Heuristic: a source is trustworthy if the (old) information we learn
from it confirms well the information we've learnt from other
trustworthy sources.

Note that we humans put a tremendous amount of time and effort into
determining credibility and trustworthiness, and this affects our
reasoning greatly.  If a chain of reasoning depends on a statement
whose probability we assess to be intermediate or low, then we need to
check that statement more carefully.

An additional issue is that often we don't speak in literal terms.
``New York is the Capital of the World'' doesn't actually quite mean
(New York, CapitalOf, World), at least not in the same sense as
(Paris, CapitalOf, France).  And even the most trustworthy and
credible sources will often contain statements of this sort.

\textbf{What's the connection to entity-relationship diagrams, if
  anything?}

\end{document}